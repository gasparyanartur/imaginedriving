{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/home/gasparyanartur/dev/imaginedriving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gasparyanartur/dev/imaginedriving/venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/home/gasparyanartur/dev/imaginedriving/venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ~/dev/imaginedriving/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, Union, Tuple\n",
    "from diffusers.models.controlnet import ControlNetOutput, ControlNetModel\n",
    "import torch\n",
    "from src.data import read_image\n",
    "from diffusers import ControlNetModel, StableDiffusionControlNetImg2ImgPipeline, UNet2DConditionModel\n",
    "from diffusers.loaders.lora import LoraLoaderMixin\n",
    "from src.control_lora import ControlLoRAModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class PeftCompatibleControlNet(ControlNetModel):\n",
    "    def forward(\n",
    "            self,\n",
    "            sample: torch.FloatTensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            encoder_hidden_states: torch.Tensor,\n",
    "            controlnet_cond: torch.FloatTensor,\n",
    "            conditioning_scale: float = 1.0,\n",
    "            class_labels: Optional[torch.Tensor] = None,\n",
    "            timestep_cond: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            added_cond_kwargs: Optional[Dict[str, torch.Tensor]] = None,\n",
    "            cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "            guess_mode: bool = False,\n",
    "            return_dict: bool = True,\n",
    "        ) -> Union[ControlNetOutput, Tuple[Tuple[torch.FloatTensor, ...], torch.FloatTensor]]:\n",
    "            \"\"\"\n",
    "            The [`ControlNetModel`] forward method.\n",
    "\n",
    "            Args:\n",
    "                sample (`torch.FloatTensor`):\n",
    "                    The noisy input tensor.\n",
    "                timestep (`Union[torch.Tensor, float, int]`):\n",
    "                    The number of timesteps to denoise an input.\n",
    "                encoder_hidden_states (`torch.Tensor`):\n",
    "                    The encoder hidden states.\n",
    "                controlnet_cond (`torch.FloatTensor`):\n",
    "                    The conditional input tensor of shape `(batch_size, sequence_length, hidden_size)`.\n",
    "                conditioning_scale (`float`, defaults to `1.0`):\n",
    "                    The scale factor for ControlNet outputs.\n",
    "                class_labels (`torch.Tensor`, *optional*, defaults to `None`):\n",
    "                    Optional class labels for conditioning. Their embeddings will be summed with the timestep embeddings.\n",
    "                timestep_cond (`torch.Tensor`, *optional*, defaults to `None`):\n",
    "                    Additional conditional embeddings for timestep. If provided, the embeddings will be summed with the\n",
    "                    timestep_embedding passed through the `self.time_embedding` layer to obtain the final timestep\n",
    "                    embeddings.\n",
    "                attention_mask (`torch.Tensor`, *optional*, defaults to `None`):\n",
    "                    An attention mask of shape `(batch, key_tokens)` is applied to `encoder_hidden_states`. If `1` the mask\n",
    "                    is kept, otherwise if `0` it is discarded. Mask will be converted into a bias, which adds large\n",
    "                    negative values to the attention scores corresponding to \"discard\" tokens.\n",
    "                added_cond_kwargs (`dict`):\n",
    "                    Additional conditions for the Stable Diffusion XL UNet.\n",
    "                cross_attention_kwargs (`dict[str]`, *optional*, defaults to `None`):\n",
    "                    A kwargs dictionary that if specified is passed along to the `AttnProcessor`.\n",
    "                guess_mode (`bool`, defaults to `False`):\n",
    "                    In this mode, the ControlNet encoder tries its best to recognize the input content of the input even if\n",
    "                    you remove all prompts. A `guidance_scale` between 3.0 and 5.0 is recommended.\n",
    "                return_dict (`bool`, defaults to `True`):\n",
    "                    Whether or not to return a [`~models.controlnet.ControlNetOutput`] instead of a plain tuple.\n",
    "\n",
    "            Returns:\n",
    "                [`~models.controlnet.ControlNetOutput`] **or** `tuple`:\n",
    "                    If `return_dict` is `True`, a [`~models.controlnet.ControlNetOutput`] is returned, otherwise a tuple is\n",
    "                    returned where the first element is the sample tensor.\n",
    "            \"\"\"\n",
    "            # check channel order\n",
    "            channel_order = self.config.controlnet_conditioning_channel_order\n",
    "\n",
    "            if channel_order == \"rgb\":\n",
    "                # in rgb order by default\n",
    "                ...\n",
    "            elif channel_order == \"bgr\":\n",
    "                controlnet_cond = torch.flip(controlnet_cond, dims=[1])\n",
    "            else:\n",
    "                raise ValueError(f\"unknown `controlnet_conditioning_channel_order`: {channel_order}\")\n",
    "\n",
    "            # prepare attention_mask\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = (1 - attention_mask.to(sample.dtype)) * -10000.0\n",
    "                attention_mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "            # 1. time\n",
    "            timesteps = timestep\n",
    "            if not torch.is_tensor(timesteps):\n",
    "                # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "                # This would be a good case for the `match` statement (Python 3.10+)\n",
    "                is_mps = sample.device.type == \"mps\"\n",
    "                if isinstance(timestep, float):\n",
    "                    dtype = torch.float32 if is_mps else torch.float64\n",
    "                else:\n",
    "                    dtype = torch.int32 if is_mps else torch.int64\n",
    "                timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "            elif len(timesteps.shape) == 0:\n",
    "                timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "            # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "            timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "            t_emb = self.time_proj(timesteps)\n",
    "\n",
    "            # timesteps does not contain any weights and will always return f32 tensors\n",
    "            # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "            # there might be better ways to encapsulate this.\n",
    "            t_emb = t_emb.to(dtype=sample.dtype)\n",
    "\n",
    "            emb = self.time_embedding(t_emb, timestep_cond)\n",
    "            aug_emb = None\n",
    "\n",
    "            if self.class_embedding is not None:\n",
    "                if class_labels is None:\n",
    "                    raise ValueError(\"class_labels should be provided when num_class_embeds > 0\")\n",
    "\n",
    "                if self.config.class_embed_type == \"timestep\":\n",
    "                    class_labels = self.time_proj(class_labels)\n",
    "\n",
    "                class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
    "                emb = emb + class_emb\n",
    "\n",
    "            if self.config.addition_embed_type is not None:\n",
    "                if self.config.addition_embed_type == \"text\":\n",
    "                    aug_emb = self.add_embedding(encoder_hidden_states)\n",
    "\n",
    "                elif self.config.addition_embed_type == \"text_time\":\n",
    "                    if \"text_embeds\" not in added_cond_kwargs:\n",
    "                        raise ValueError(\n",
    "                            f\"{self.__class__} has the config param `addition_embed_type` set to 'text_time' which requires the keyword argument `text_embeds` to be passed in `added_cond_kwargs`\"\n",
    "                        )\n",
    "                    text_embeds = added_cond_kwargs.get(\"text_embeds\")\n",
    "                    if \"time_ids\" not in added_cond_kwargs:\n",
    "                        raise ValueError(\n",
    "                            f\"{self.__class__} has the config param `addition_embed_type` set to 'text_time' which requires the keyword argument `time_ids` to be passed in `added_cond_kwargs`\"\n",
    "                        )\n",
    "                    time_ids = added_cond_kwargs.get(\"time_ids\")\n",
    "                    time_embeds = self.add_time_proj(time_ids.flatten())\n",
    "                    time_embeds = time_embeds.reshape((text_embeds.shape[0], -1))\n",
    "\n",
    "                    add_embeds = torch.concat([text_embeds, time_embeds], dim=-1)\n",
    "                    add_embeds = add_embeds.to(emb.dtype)\n",
    "                    aug_emb = self.add_embedding(add_embeds)\n",
    "\n",
    "            emb = emb + aug_emb if aug_emb is not None else emb\n",
    "\n",
    "            # 2. pre-process\n",
    "            sample = self.conv_in(sample)\n",
    "\n",
    "            controlnet_cond = self.controlnet_cond_embedding(controlnet_cond)\n",
    "            sample = sample + controlnet_cond\n",
    "\n",
    "            # 3. down\n",
    "            down_block_res_samples = (sample,)\n",
    "            for downsample_block in self.down_blocks:\n",
    "                if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                    sample, res_samples = downsample_block(\n",
    "                        hidden_states=sample,\n",
    "                        temb=emb,\n",
    "                        encoder_hidden_states=encoder_hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    )\n",
    "                else:\n",
    "                    sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "                down_block_res_samples += res_samples\n",
    "\n",
    "            # 4. mid\n",
    "            if self.mid_block is not None:\n",
    "                if hasattr(self.mid_block, \"has_cross_attention\") and self.mid_block.has_cross_attention:\n",
    "                    sample = self.mid_block(\n",
    "                        sample,\n",
    "                        emb,\n",
    "                        encoder_hidden_states=encoder_hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    )\n",
    "                else:\n",
    "                    sample = self.mid_block(sample, emb)\n",
    "\n",
    "            # 5. Control net blocks\n",
    "\n",
    "            controlnet_down_block_res_samples = ()\n",
    "\n",
    "            #controlnet_down_blocks = next((b for a, b in self.controlnet_down_blocks.named_children() if a == \"modules_to_save\"))[\"default\"]\n",
    "            controlnet_down_blocks = next((b for a, b in self.controlnet_down_blocks.named_children() if a == \"original_module\"))\n",
    "            for down_block_res_sample, controlnet_block in zip(down_block_res_samples, controlnet_down_blocks):\n",
    "                down_block_res_sample = controlnet_block(down_block_res_sample)\n",
    "                controlnet_down_block_res_samples = controlnet_down_block_res_samples + (down_block_res_sample,)\n",
    "\n",
    "            down_block_res_samples = controlnet_down_block_res_samples\n",
    "\n",
    "            mid_block_res_sample = self.controlnet_mid_block(sample)\n",
    "\n",
    "            # 6. scaling\n",
    "            if guess_mode and not self.config.global_pool_conditions:\n",
    "                scales = torch.logspace(-1, 0, len(down_block_res_samples) + 1, device=sample.device)  # 0.1 to 1.0\n",
    "                scales = scales * conditioning_scale\n",
    "                down_block_res_samples = [sample * scale for sample, scale in zip(down_block_res_samples, scales)]\n",
    "                mid_block_res_sample = mid_block_res_sample * scales[-1]  # last one\n",
    "            else:\n",
    "                down_block_res_samples = [sample * conditioning_scale for sample in down_block_res_samples]\n",
    "                mid_block_res_sample = mid_block_res_sample * conditioning_scale\n",
    "\n",
    "            if self.config.global_pool_conditions:\n",
    "                down_block_res_samples = [\n",
    "                    torch.mean(sample, dim=(2, 3), keepdim=True) for sample in down_block_res_samples\n",
    "                ]\n",
    "                mid_block_res_sample = torch.mean(mid_block_res_sample, dim=(2, 3), keepdim=True)\n",
    "\n",
    "            if not return_dict:\n",
    "                return (down_block_res_samples, mid_block_res_sample)\n",
    "\n",
    "            return ControlNetOutput(\n",
    "                down_block_res_samples=down_block_res_samples, mid_block_res_sample=mid_block_res_sample\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gasparyanartur/dev/imaginedriving/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870399bee68e4b06ae2a2dc5c3e2a719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "controlnet = PeftCompatibleControlNet.from_unet(unet)\n",
    "pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
    "    model_id, unet=unet, controlnet=controlnet, torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet2DConditionModel(\n",
       "  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock2DCrossAttn(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftCompatibleControlNet(\n",
       "  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (controlnet_cond_embedding): ControlNetConditioningEmbedding(\n",
       "    (conv_in): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (blocks): ModuleList(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): Conv2d(96, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (conv_out): Conv2d(256, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (controlnet_down_blocks): ModuleList(\n",
       "    (0-3): 4 x Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4-6): 3 x Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (7-11): 5 x Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (controlnet_mid_block): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (mid_block): UNetMidBlock2DCrossAttn(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.controlnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',\n",
       "  UNet2DConditionModel(\n",
       "    (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_proj): Timesteps()\n",
       "    (time_embedding): TimestepEmbedding(\n",
       "      (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (act): SiLU()\n",
       "      (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): CrossAttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossAttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossAttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossAttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2DCrossAttn(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )),\n",
       " ('conv_in',\n",
       "  Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('time_proj', Timesteps()),\n",
       " ('time_embedding',\n",
       "  TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('time_embedding.linear_1',\n",
       "  Linear(in_features=320, out_features=1280, bias=True)),\n",
       " ('time_embedding.act', SiLU()),\n",
       " ('time_embedding.linear_2',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks',\n",
       "  ModuleList(\n",
       "    (0): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0',\n",
       "  CrossAttnDownBlock2D(\n",
       "    (attentions): ModuleList(\n",
       "      (0-1): 2 x Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (downsamplers): ModuleList(\n",
       "      (0): Downsample2D(\n",
       "        (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions',\n",
       "  ModuleList(\n",
       "    (0-1): 2 x Transformer2DModel(\n",
       "      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "      (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn2): Attention(\n",
       "            (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "            (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0.norm',\n",
       "  GroupNorm(32, 320, eps=1e-06, affine=True)),\n",
       " ('down_blocks.0.attentions.0.proj_in',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.norm1',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.norm2',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.norm3',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "  )),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=320, out_features=2560, bias=True)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=1280, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.0.proj_out',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.1',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "  )),\n",
       " ('down_blocks.0.attentions.1.norm',\n",
       "  GroupNorm(32, 320, eps=1e-06, affine=True)),\n",
       " ('down_blocks.0.attentions.1.proj_in',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.norm1',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.norm2',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.norm3',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "  )),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "  )),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=320, out_features=2560, bias=True)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=1280, out_features=320, bias=True)),\n",
       " ('down_blocks.0.attentions.1.proj_out',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('down_blocks.0.resnets',\n",
       "  ModuleList(\n",
       "    (0-1): 2 x ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.resnets.0',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "    (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "  )),\n",
       " ('down_blocks.0.resnets.0.norm1', GroupNorm(32, 320, eps=1e-05, affine=True)),\n",
       " ('down_blocks.0.resnets.0.conv1',\n",
       "  Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.0.resnets.0.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=320, bias=True)),\n",
       " ('down_blocks.0.resnets.0.norm2', GroupNorm(32, 320, eps=1e-05, affine=True)),\n",
       " ('down_blocks.0.resnets.0.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.0.resnets.0.conv2',\n",
       "  Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.0.resnets.1',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "    (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "  )),\n",
       " ('down_blocks.0.resnets.1.norm1', GroupNorm(32, 320, eps=1e-05, affine=True)),\n",
       " ('down_blocks.0.resnets.1.conv1',\n",
       "  Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.0.resnets.1.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=320, bias=True)),\n",
       " ('down_blocks.0.resnets.1.norm2', GroupNorm(32, 320, eps=1e-05, affine=True)),\n",
       " ('down_blocks.0.resnets.1.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.0.resnets.1.conv2',\n",
       "  Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.0.downsamplers',\n",
       "  ModuleList(\n",
       "    (0): Downsample2D(\n",
       "      (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.0.downsamplers.0',\n",
       "  Downsample2D(\n",
       "    (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )),\n",
       " ('down_blocks.0.downsamplers.0.conv',\n",
       "  Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))),\n",
       " ('down_blocks.1',\n",
       "  CrossAttnDownBlock2D(\n",
       "    (attentions): ModuleList(\n",
       "      (0-1): 2 x Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0): ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (downsamplers): ModuleList(\n",
       "      (0): Downsample2D(\n",
       "        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions',\n",
       "  ModuleList(\n",
       "    (0-1): 2 x Transformer2DModel(\n",
       "      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "      (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn2): Attention(\n",
       "            (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "            (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0.norm',\n",
       "  GroupNorm(32, 640, eps=1e-06, affine=True)),\n",
       " ('down_blocks.1.attentions.0.proj_in',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.norm1',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.norm2',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.norm3',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "  )),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=640, out_features=5120, bias=True)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=2560, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.0.proj_out',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.1',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "  )),\n",
       " ('down_blocks.1.attentions.1.norm',\n",
       "  GroupNorm(32, 640, eps=1e-06, affine=True)),\n",
       " ('down_blocks.1.attentions.1.proj_in',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.norm1',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.norm2',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.norm3',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "  )),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "  )),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=640, out_features=5120, bias=True)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=2560, out_features=640, bias=True)),\n",
       " ('down_blocks.1.attentions.1.proj_out',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('down_blocks.1.resnets',\n",
       "  ModuleList(\n",
       "    (0): ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "      (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "      (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.resnets.0',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "    (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('down_blocks.1.resnets.0.norm1', GroupNorm(32, 320, eps=1e-05, affine=True)),\n",
       " ('down_blocks.1.resnets.0.conv1',\n",
       "  Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.1.resnets.0.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=640, bias=True)),\n",
       " ('down_blocks.1.resnets.0.norm2', GroupNorm(32, 640, eps=1e-05, affine=True)),\n",
       " ('down_blocks.1.resnets.0.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.1.resnets.0.conv2',\n",
       "  Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.1.resnets.0.conv_shortcut',\n",
       "  Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('down_blocks.1.resnets.1',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "    (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "  )),\n",
       " ('down_blocks.1.resnets.1.norm1', GroupNorm(32, 640, eps=1e-05, affine=True)),\n",
       " ('down_blocks.1.resnets.1.conv1',\n",
       "  Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.1.resnets.1.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=640, bias=True)),\n",
       " ('down_blocks.1.resnets.1.norm2', GroupNorm(32, 640, eps=1e-05, affine=True)),\n",
       " ('down_blocks.1.resnets.1.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.1.resnets.1.conv2',\n",
       "  Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.1.downsamplers',\n",
       "  ModuleList(\n",
       "    (0): Downsample2D(\n",
       "      (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.1.downsamplers.0',\n",
       "  Downsample2D(\n",
       "    (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )),\n",
       " ('down_blocks.1.downsamplers.0.conv',\n",
       "  Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))),\n",
       " ('down_blocks.2',\n",
       "  CrossAttnDownBlock2D(\n",
       "    (attentions): ModuleList(\n",
       "      (0-1): 2 x Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0): ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (downsamplers): ModuleList(\n",
       "      (0): Downsample2D(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions',\n",
       "  ModuleList(\n",
       "    (0-1): 2 x Transformer2DModel(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn2): Attention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0.norm',\n",
       "  GroupNorm(32, 1280, eps=1e-06, affine=True)),\n",
       " ('down_blocks.2.attentions.0.proj_in',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.norm1',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.norm2',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.norm3',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "  )),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=1280, out_features=10240, bias=True)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=5120, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.0.proj_out',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.1',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('down_blocks.2.attentions.1.norm',\n",
       "  GroupNorm(32, 1280, eps=1e-06, affine=True)),\n",
       " ('down_blocks.2.attentions.1.proj_in',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.norm1',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.norm2',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.norm3',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "  )),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=1280, out_features=10240, bias=True)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=5120, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.attentions.1.proj_out',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.resnets',\n",
       "  ModuleList(\n",
       "    (0): ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.resnets.0',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('down_blocks.2.resnets.0.norm1', GroupNorm(32, 640, eps=1e-05, affine=True)),\n",
       " ('down_blocks.2.resnets.0.conv1',\n",
       "  Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.2.resnets.0.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.resnets.0.norm2',\n",
       "  GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('down_blocks.2.resnets.0.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.2.resnets.0.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.2.resnets.0.conv_shortcut',\n",
       "  Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('down_blocks.2.resnets.1',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "  )),\n",
       " ('down_blocks.2.resnets.1.norm1',\n",
       "  GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('down_blocks.2.resnets.1.conv1',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.2.resnets.1.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.2.resnets.1.norm2',\n",
       "  GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('down_blocks.2.resnets.1.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.2.resnets.1.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.2.downsamplers',\n",
       "  ModuleList(\n",
       "    (0): Downsample2D(\n",
       "      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.2.downsamplers.0',\n",
       "  Downsample2D(\n",
       "    (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )),\n",
       " ('down_blocks.2.downsamplers.0.conv',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))),\n",
       " ('down_blocks.3',\n",
       "  DownBlock2D(\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.3.resnets',\n",
       "  ModuleList(\n",
       "    (0-1): 2 x ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "    )\n",
       "  )),\n",
       " ('down_blocks.3.resnets.0',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "  )),\n",
       " ('down_blocks.3.resnets.0.norm1',\n",
       "  GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('down_blocks.3.resnets.0.conv1',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.3.resnets.0.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.3.resnets.0.norm2',\n",
       "  GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('down_blocks.3.resnets.0.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.3.resnets.0.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.3.resnets.1',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "  )),\n",
       " ('down_blocks.3.resnets.1.norm1',\n",
       "  GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('down_blocks.3.resnets.1.conv1',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('down_blocks.3.resnets.1.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('down_blocks.3.resnets.1.norm2',\n",
       "  GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('down_blocks.3.resnets.1.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('down_blocks.3.resnets.1.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks',\n",
       "  ModuleList(\n",
       "    (0): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.0',\n",
       "  UpBlock2D(\n",
       "    (resnets): ModuleList(\n",
       "      (0-2): 3 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (upsamplers): ModuleList(\n",
       "      (0): Upsample2D(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.0.resnets',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.0.resnets.0',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.0.resnets.0.norm1', GroupNorm(32, 2560, eps=1e-05, affine=True)),\n",
       " ('up_blocks.0.resnets.0.conv1',\n",
       "  Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.0.resnets.0.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.0.resnets.0.norm2', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('up_blocks.0.resnets.0.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.0.resnets.0.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.0.resnets.0.conv_shortcut',\n",
       "  Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.0.resnets.1',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.0.resnets.1.norm1', GroupNorm(32, 2560, eps=1e-05, affine=True)),\n",
       " ('up_blocks.0.resnets.1.conv1',\n",
       "  Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.0.resnets.1.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.0.resnets.1.norm2', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('up_blocks.0.resnets.1.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.0.resnets.1.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.0.resnets.1.conv_shortcut',\n",
       "  Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.0.resnets.2',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.0.resnets.2.norm1', GroupNorm(32, 2560, eps=1e-05, affine=True)),\n",
       " ('up_blocks.0.resnets.2.conv1',\n",
       "  Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.0.resnets.2.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.0.resnets.2.norm2', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('up_blocks.0.resnets.2.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.0.resnets.2.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.0.resnets.2.conv_shortcut',\n",
       "  Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.0.upsamplers',\n",
       "  ModuleList(\n",
       "    (0): Upsample2D(\n",
       "      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.0.upsamplers.0',\n",
       "  Upsample2D(\n",
       "    (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.0.upsamplers.0.conv',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.1',\n",
       "  CrossAttnUpBlock2D(\n",
       "    (attentions): ModuleList(\n",
       "      (0-2): 3 x Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (upsamplers): ModuleList(\n",
       "      (0): Upsample2D(\n",
       "        (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x Transformer2DModel(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn2): Attention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0.norm',\n",
       "  GroupNorm(32, 1280, eps=1e-06, affine=True)),\n",
       " ('up_blocks.1.attentions.0.proj_in',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.norm1',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.norm2',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.norm3',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=1280, out_features=10240, bias=True)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=5120, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.0.proj_out',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.1',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.1.norm',\n",
       "  GroupNorm(32, 1280, eps=1e-06, affine=True)),\n",
       " ('up_blocks.1.attentions.1.proj_in',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.norm1',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.norm2',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.norm3',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=1280, out_features=10240, bias=True)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=5120, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.1.proj_out',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.2',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.2.norm',\n",
       "  GroupNorm(32, 1280, eps=1e-06, affine=True)),\n",
       " ('up_blocks.1.attentions.2.proj_in',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.norm1',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.norm2',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.norm3',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "  )),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=1280, out_features=10240, bias=True)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=5120, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.attentions.2.proj_out',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.resnets',\n",
       "  ModuleList(\n",
       "    (0-1): 2 x ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.resnets.0',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.1.resnets.0.norm1', GroupNorm(32, 2560, eps=1e-05, affine=True)),\n",
       " ('up_blocks.1.resnets.0.conv1',\n",
       "  Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.1.resnets.0.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.resnets.0.norm2', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('up_blocks.1.resnets.0.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.resnets.0.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.1.resnets.0.conv_shortcut',\n",
       "  Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.1.resnets.1',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.1.resnets.1.norm1', GroupNorm(32, 2560, eps=1e-05, affine=True)),\n",
       " ('up_blocks.1.resnets.1.conv1',\n",
       "  Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.1.resnets.1.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.resnets.1.norm2', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('up_blocks.1.resnets.1.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.resnets.1.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.1.resnets.1.conv_shortcut',\n",
       "  Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.1.resnets.2',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.1.resnets.2.norm1', GroupNorm(32, 1920, eps=1e-05, affine=True)),\n",
       " ('up_blocks.1.resnets.2.conv1',\n",
       "  Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.1.resnets.2.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('up_blocks.1.resnets.2.norm2', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('up_blocks.1.resnets.2.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.1.resnets.2.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.1.resnets.2.conv_shortcut',\n",
       "  Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.1.upsamplers',\n",
       "  ModuleList(\n",
       "    (0): Upsample2D(\n",
       "      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.1.upsamplers.0',\n",
       "  Upsample2D(\n",
       "    (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.1.upsamplers.0.conv',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.2',\n",
       "  CrossAttnUpBlock2D(\n",
       "    (attentions): ModuleList(\n",
       "      (0-2): 3 x Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "        (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "              (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "              (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0): ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (upsamplers): ModuleList(\n",
       "      (0): Upsample2D(\n",
       "        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x Transformer2DModel(\n",
       "      (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "      (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn2): Attention(\n",
       "            (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "            (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "            (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0.norm', GroupNorm(32, 640, eps=1e-06, affine=True)),\n",
       " ('up_blocks.2.attentions.0.proj_in',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.norm1',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.norm2',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.norm3',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=640, out_features=5120, bias=True)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=2560, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.0.proj_out',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.1',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.1.norm', GroupNorm(32, 640, eps=1e-06, affine=True)),\n",
       " ('up_blocks.2.attentions.1.proj_in',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.norm1',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.norm2',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.norm3',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=640, out_features=5120, bias=True)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=2560, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.1.proj_out',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.2',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.2.norm', GroupNorm(32, 640, eps=1e-06, affine=True)),\n",
       " ('up_blocks.2.attentions.2.proj_in',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.norm1',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.norm2',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=640, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=640, bias=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.norm3',\n",
       "  LayerNorm((640,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "  )),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=640, out_features=5120, bias=True)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=2560, out_features=640, bias=True)),\n",
       " ('up_blocks.2.attentions.2.proj_out',\n",
       "  Linear(in_features=640, out_features=640, bias=True)),\n",
       " ('up_blocks.2.resnets',\n",
       "  ModuleList(\n",
       "    (0): ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "      (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "      (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "      (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.resnets.0',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "    (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.2.resnets.0.norm1', GroupNorm(32, 1920, eps=1e-05, affine=True)),\n",
       " ('up_blocks.2.resnets.0.conv1',\n",
       "  Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.2.resnets.0.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=640, bias=True)),\n",
       " ('up_blocks.2.resnets.0.norm2', GroupNorm(32, 640, eps=1e-05, affine=True)),\n",
       " ('up_blocks.2.resnets.0.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.resnets.0.conv2',\n",
       "  Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.2.resnets.0.conv_shortcut',\n",
       "  Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.2.resnets.1',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "    (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.2.resnets.1.norm1', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('up_blocks.2.resnets.1.conv1',\n",
       "  Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.2.resnets.1.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=640, bias=True)),\n",
       " ('up_blocks.2.resnets.1.norm2', GroupNorm(32, 640, eps=1e-05, affine=True)),\n",
       " ('up_blocks.2.resnets.1.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.resnets.1.conv2',\n",
       "  Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.2.resnets.1.conv_shortcut',\n",
       "  Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.2.resnets.2',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "    (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.2.resnets.2.norm1', GroupNorm(32, 960, eps=1e-05, affine=True)),\n",
       " ('up_blocks.2.resnets.2.conv1',\n",
       "  Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.2.resnets.2.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=640, bias=True)),\n",
       " ('up_blocks.2.resnets.2.norm2', GroupNorm(32, 640, eps=1e-05, affine=True)),\n",
       " ('up_blocks.2.resnets.2.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.2.resnets.2.conv2',\n",
       "  Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.2.resnets.2.conv_shortcut',\n",
       "  Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.2.upsamplers',\n",
       "  ModuleList(\n",
       "    (0): Upsample2D(\n",
       "      (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.2.upsamplers.0',\n",
       "  Upsample2D(\n",
       "    (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.2.upsamplers.0.conv',\n",
       "  Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.3',\n",
       "  CrossAttnUpBlock2D(\n",
       "    (attentions): ModuleList(\n",
       "      (0-2): 3 x Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "        (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "              (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "              (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0): ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1-2): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "        (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x Transformer2DModel(\n",
       "      (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "      (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn2): Attention(\n",
       "            (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "            (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "            (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0.norm', GroupNorm(32, 320, eps=1e-06, affine=True)),\n",
       " ('up_blocks.3.attentions.0.proj_in',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.norm1',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.norm2',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.norm3',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=320, out_features=2560, bias=True)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=1280, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.0.proj_out',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.1',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.1.norm', GroupNorm(32, 320, eps=1e-06, affine=True)),\n",
       " ('up_blocks.3.attentions.1.proj_in',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.norm1',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.norm2',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.norm3',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=320, out_features=2560, bias=True)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=1280, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.1.proj_out',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.2',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.2.norm', GroupNorm(32, 320, eps=1e-06, affine=True)),\n",
       " ('up_blocks.3.attentions.2.proj_in',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.norm1',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.norm2',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=320, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=320, bias=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.norm3',\n",
       "  LayerNorm((320,), eps=1e-05, elementwise_affine=True)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "  )),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=320, out_features=2560, bias=True)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=1280, out_features=320, bias=True)),\n",
       " ('up_blocks.3.attentions.2.proj_out',\n",
       "  Linear(in_features=320, out_features=320, bias=True)),\n",
       " ('up_blocks.3.resnets',\n",
       "  ModuleList(\n",
       "    (0): ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1-2): 2 x ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "      (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )),\n",
       " ('up_blocks.3.resnets.0',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "    (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.3.resnets.0.norm1', GroupNorm(32, 960, eps=1e-05, affine=True)),\n",
       " ('up_blocks.3.resnets.0.conv1',\n",
       "  Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.3.resnets.0.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=320, bias=True)),\n",
       " ('up_blocks.3.resnets.0.norm2', GroupNorm(32, 320, eps=1e-05, affine=True)),\n",
       " ('up_blocks.3.resnets.0.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.resnets.0.conv2',\n",
       "  Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.3.resnets.0.conv_shortcut',\n",
       "  Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.3.resnets.1',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "    (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.3.resnets.1.norm1', GroupNorm(32, 640, eps=1e-05, affine=True)),\n",
       " ('up_blocks.3.resnets.1.conv1',\n",
       "  Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.3.resnets.1.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=320, bias=True)),\n",
       " ('up_blocks.3.resnets.1.norm2', GroupNorm(32, 320, eps=1e-05, affine=True)),\n",
       " ('up_blocks.3.resnets.1.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.resnets.1.conv2',\n",
       "  Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.3.resnets.1.conv_shortcut',\n",
       "  Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('up_blocks.3.resnets.2',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "    (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "    (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )),\n",
       " ('up_blocks.3.resnets.2.norm1', GroupNorm(32, 640, eps=1e-05, affine=True)),\n",
       " ('up_blocks.3.resnets.2.conv1',\n",
       "  Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.3.resnets.2.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=320, bias=True)),\n",
       " ('up_blocks.3.resnets.2.norm2', GroupNorm(32, 320, eps=1e-05, affine=True)),\n",
       " ('up_blocks.3.resnets.2.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('up_blocks.3.resnets.2.conv2',\n",
       "  Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('up_blocks.3.resnets.2.conv_shortcut',\n",
       "  Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))),\n",
       " ('mid_block',\n",
       "  UNetMidBlock2DCrossAttn(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('mid_block.attentions',\n",
       "  ModuleList(\n",
       "    (0): Transformer2DModel(\n",
       "      (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "      (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (transformer_blocks): ModuleList(\n",
       "        (0): BasicTransformerBlock(\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn1): Attention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn2): Attention(\n",
       "            (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "            (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (net): ModuleList(\n",
       "              (0): GEGLU(\n",
       "                (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "              )\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "              (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('mid_block.attentions.0',\n",
       "  Transformer2DModel(\n",
       "    (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "    (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): BasicTransformerBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn1): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn2): Attention(\n",
       "          (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): FeedForward(\n",
       "          (net): ModuleList(\n",
       "            (0): GEGLU(\n",
       "              (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('mid_block.attentions.0.norm', GroupNorm(32, 1280, eps=1e-06, affine=True)),\n",
       " ('mid_block.attentions.0.proj_in',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('mid_block.attentions.0.transformer_blocks',\n",
       "  ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn2): Attention(\n",
       "        (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('mid_block.attentions.0.transformer_blocks.0',\n",
       "  BasicTransformerBlock(\n",
       "    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn1): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn2): Attention(\n",
       "      (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "      (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "      (to_out): ModuleList(\n",
       "        (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): FeedForward(\n",
       "      (net): ModuleList(\n",
       "        (0): GEGLU(\n",
       "          (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.norm1',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1.to_k',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1.to_v',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn1.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.norm2',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2',\n",
       "  Attention(\n",
       "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "    (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "    (to_out): ModuleList(\n",
       "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2.to_q',\n",
       "  Linear(in_features=1280, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2.to_k',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2.to_v',\n",
       "  Linear(in_features=1024, out_features=1280, bias=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2.to_out',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.attn2.to_out.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.norm3',\n",
       "  LayerNorm((1280,), eps=1e-05, elementwise_affine=True)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.ff',\n",
       "  FeedForward(\n",
       "    (net): ModuleList(\n",
       "      (0): GEGLU(\n",
       "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "      )\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.ff.net',\n",
       "  ModuleList(\n",
       "    (0): GEGLU(\n",
       "      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "    )\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "  )),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.ff.net.0',\n",
       "  GEGLU(\n",
       "    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "  )),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj',\n",
       "  Linear(in_features=1280, out_features=10240, bias=True)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.ff.net.1',\n",
       "  Dropout(p=0.0, inplace=False)),\n",
       " ('mid_block.attentions.0.transformer_blocks.0.ff.net.2',\n",
       "  Linear(in_features=5120, out_features=1280, bias=True)),\n",
       " ('mid_block.attentions.0.proj_out',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('mid_block.resnets',\n",
       "  ModuleList(\n",
       "    (0-1): 2 x ResnetBlock2D(\n",
       "      (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (nonlinearity): SiLU()\n",
       "    )\n",
       "  )),\n",
       " ('mid_block.resnets.0',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "  )),\n",
       " ('mid_block.resnets.0.norm1', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('mid_block.resnets.0.conv1',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('mid_block.resnets.0.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('mid_block.resnets.0.norm2', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('mid_block.resnets.0.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('mid_block.resnets.0.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('mid_block.resnets.1',\n",
       "  ResnetBlock2D(\n",
       "    (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (nonlinearity): SiLU()\n",
       "  )),\n",
       " ('mid_block.resnets.1.norm1', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('mid_block.resnets.1.conv1',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('mid_block.resnets.1.time_emb_proj',\n",
       "  Linear(in_features=1280, out_features=1280, bias=True)),\n",
       " ('mid_block.resnets.1.norm2', GroupNorm(32, 1280, eps=1e-05, affine=True)),\n",
       " ('mid_block.resnets.1.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('mid_block.resnets.1.conv2',\n",
       "  Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       " ('conv_norm_out', GroupNorm(32, 320, eps=1e-05, affine=True)),\n",
       " ('conv_out',\n",
       "  Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Union, Iterable\n",
    "import re\n",
    "\n",
    "def get_matching(model, patterns: Iterable[Union[re.Pattern, str]] = (\".*\",)):\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        if isinstance(pattern, str):\n",
    "            patterns[i] = re.compile(pattern)\n",
    "\n",
    "    li = []\n",
    "    for name, mod in model.named_modules():\n",
    "        for pattern in patterns:\n",
    "            if pattern.match(name):\n",
    "                li.append((name, mod))\n",
    "    return li\n",
    "\n",
    "\n",
    "get_matching(pipe.unet, [r\".*\\.to_[qkv]\", r\".*\\.to_out.0\", r\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unet': {'.*down_blocks.*attn.*to_[kvq]': 4,\n",
       "  '.*down_blocks.*attn.*to_out\\\\.0': 4,\n",
       "  '.*down_blocks.*resnets.*conv\\\\d*': 4,\n",
       "  '.*down_blocks.*resnets.*time_emb_proj': 4,\n",
       "  '.*down_blocks.*ff\\\\.net\\\\.0\\\\.proj': 8,\n",
       "  '.*down_blocks.*ff\\\\.net\\\\.2': 8,\n",
       "  '.*down_blocks.*attentions.*proj_in': 8,\n",
       "  '.*down_blocks.*attentions.*proj_out': 8,\n",
       "  '.*mid_blocks.*attn.*to_[kvq]': 8,\n",
       "  '.*mid_blocks.*attn.*to_out\\\\.0': 8,\n",
       "  '.*mid_blocks.*resnets.*conv\\\\d*': 8,\n",
       "  '.*mid_blocks.*resnets.*time_emb_proj': 8,\n",
       "  '.*mid_blocks.*ff\\\\.net\\\\.0\\\\.proj': 16,\n",
       "  '.*mid_blocks.*ff\\\\.net\\\\.2': 16,\n",
       "  '.*mid_blocks.*attentions.*proj_in': 16,\n",
       "  '.*mid_blocks.*attentions.*proj_out': 16,\n",
       "  '.*up_blocks.*attn.*to_[kvq]': 8,\n",
       "  '.*up_blocks.*attn.*to_out\\\\.0': 8,\n",
       "  '.*up_blocks.*resnets.*conv\\\\d*': 8,\n",
       "  '.*up_blocks.*resnets.*time_emb_proj': 8,\n",
       "  '.*up_blocks.*ff\\\\.net\\\\.0\\\\.proj': 16,\n",
       "  '.*up_blocks.*ff\\\\.net\\\\.2': 16,\n",
       "  '.*up_blocks.*attentions.*proj_in': 16,\n",
       "  '.*up_blocks.*attentions.*proj_out': 16},\n",
       " 'controlnet': {'.*down_blocks.*attn.*to_[kvq]': 8,\n",
       "  '.*down_blocks.*attn.*to_out\\\\.0': 8,\n",
       "  '.*down_blocks.*resnets.*conv\\\\d*': 8,\n",
       "  '.*down_blocks.*resnets.*time_emb_proj': 8,\n",
       "  '.*down_blocks.*ff\\\\.net\\\\.0\\\\.proj': 16,\n",
       "  '.*down_blocks.*ff\\\\.net\\\\.2': 16,\n",
       "  '.*down_blocks.*attentions.*proj_in': 16,\n",
       "  '.*down_blocks.*attentions.*proj_out': 16,\n",
       "  '.*mid_blocks.*attn.*to_[kvq]': 8,\n",
       "  '.*mid_blocks.*attn.*to_out\\\\.0': 8,\n",
       "  '.*mid_blocks.*resnets.*conv\\\\d*': 8,\n",
       "  '.*mid_blocks.*resnets.*time_emb_proj': 8,\n",
       "  '.*mid_blocks.*ff\\\\.net\\\\.0\\\\.proj': 16,\n",
       "  '.*mid_blocks.*ff\\\\.net\\\\.2': 16,\n",
       "  '.*mid_blocks.*attentions.*proj_in': 16,\n",
       "  '.*mid_blocks.*attentions.*proj_out': 16}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_target_ranks = {\n",
    "    \"downblocks\": {\"attn\": 4, \"resnet\": 4, \"ff\": 8, \"proj\": 8},\n",
    "    \"midblocks\": {\"attn\": 8, \"resnet\": 8, \"ff\": 16, \"proj\": 16},\n",
    "    \"upblocks\": {\"attn\": 8, \"resnet\": 8, \"ff\": 16, \"proj\": 16},\n",
    "}\n",
    "\n",
    "controlnet_target_ranks = {\n",
    "    \"downblocks\": {\"attn\": 8, \"resnet\": 8, \"ff\": 16, \"proj\": 16},\n",
    "    \"midblocks\": {\"attn\": 8, \"resnet\": 8, \"ff\": 16, \"proj\": 16},\n",
    "}\n",
    "\n",
    "\n",
    "def parse_target_ranks(target_ranks, prefix=r\"\"):\n",
    "    parsed_targets = {}\n",
    "\n",
    "    for name, item in target_ranks.items():\n",
    "        if not item:\n",
    "            continue\n",
    "\n",
    "        match name:\n",
    "            case \"\":\n",
    "                continue\n",
    "\n",
    "            case \"downblocks\":\n",
    "                assert isinstance(item, dict)\n",
    "                parsed_targets.update(\n",
    "                    parse_target_ranks(item, rf\"{prefix}.*down_blocks\")\n",
    "                )\n",
    "\n",
    "            case \"midblocks\":\n",
    "                assert isinstance(item, dict)\n",
    "                parsed_targets.update(\n",
    "                    parse_target_ranks(item, rf\"{prefix}.*mid_blocks\")\n",
    "                )\n",
    "\n",
    "            case \"upblocks\":\n",
    "                assert isinstance(item, dict)\n",
    "                parsed_targets.update(\n",
    "                    parse_target_ranks(item, rf\"{prefix}.*up_blocks\")\n",
    "                )\n",
    "\n",
    "            case \"attn\":\n",
    "                assert isinstance(item, int)\n",
    "                parsed_targets[f\"{prefix}.*attn.*to_[kvq]\"] = item\n",
    "                parsed_targets[ rf\"{prefix}.*attn.*to_out\\.0\"] = item\n",
    "\n",
    "\n",
    "            case \"resnet\":\n",
    "                assert isinstance(item, int)\n",
    "                parsed_targets[rf\"{prefix}.*resnets.*conv\\d*\"] = item\n",
    "                parsed_targets[rf\"{prefix}.*resnets.*time_emb_proj\"] = item\n",
    "\n",
    "            case \"ff\":\n",
    "                assert isinstance(item, int)\n",
    "                parsed_targets[rf\"{prefix}.*ff\\.net\\.0\\.proj\"] = item\n",
    "                parsed_targets[rf\"{prefix}.*ff\\.net\\.2\"] = item\n",
    "\n",
    "            case \"proj\":\n",
    "                assert isinstance(item, int)\n",
    "                parsed_targets[rf\"{prefix}.*attentions.*proj_in\"] = item\n",
    "                parsed_targets[rf\"{prefix}.*attentions.*proj_out\"] = item\n",
    "\n",
    "            case \"_\":\n",
    "                raise NotImplementedError(f\"Unrecognized target: {name}\")\n",
    "\n",
    "    return parsed_targets\n",
    "\n",
    "\n",
    "unet_ranks = parse_target_ranks(unet_target_ranks)\n",
    "controlnet_ranks = parse_target_ranks(controlnet_target_ranks)\n",
    "\n",
    "parsed_ranks = {\"unet\": unet_ranks, \"controlnet\": controlnet_ranks}\n",
    "parsed_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.*down_blocks.*attn.*to_[kvq]',\n",
       " '.*down_blocks.*attn.*to_out\\\\.0',\n",
       " '.*down_blocks.*resnets.*conv\\\\d*',\n",
       " '.*down_blocks.*resnets.*time_emb_proj',\n",
       " '.*down_blocks.*ff\\\\.net\\\\.0\\\\.proj',\n",
       " '.*down_blocks.*ff\\\\.net\\\\.2',\n",
       " '.*down_blocks.*attentions.*proj_in',\n",
       " '.*down_blocks.*attentions.*proj_out',\n",
       " '.*mid_blocks.*attn.*to_[kvq]',\n",
       " '.*mid_blocks.*attn.*to_out\\\\.0',\n",
       " '.*mid_blocks.*resnets.*conv\\\\d*',\n",
       " '.*mid_blocks.*resnets.*time_emb_proj',\n",
       " '.*mid_blocks.*ff\\\\.net\\\\.0\\\\.proj',\n",
       " '.*mid_blocks.*ff\\\\.net\\\\.2',\n",
       " '.*mid_blocks.*attentions.*proj_in',\n",
       " '.*mid_blocks.*attentions.*proj_out',\n",
       " '.*up_blocks.*attn.*to_[kvq]',\n",
       " '.*up_blocks.*attn.*to_out\\\\.0',\n",
       " '.*up_blocks.*resnets.*conv\\\\d*',\n",
       " '.*up_blocks.*resnets.*time_emb_proj',\n",
       " '.*up_blocks.*ff\\\\.net\\\\.0\\\\.proj',\n",
       " '.*up_blocks.*ff\\\\.net\\\\.2',\n",
       " '.*up_blocks.*attentions.*proj_in',\n",
       " '.*up_blocks.*attentions.*proj_out']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(unet_ranks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, inject_adapter_in_model\n",
    "\n",
    "\n",
    "peft_unet_conf = LoraConfig(\n",
    "    r=8,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=\"|\".join(unet_ranks.keys()),\n",
    "    rank_pattern=unet_ranks\n",
    ")\n",
    "\n",
    "peft_controlnet_conf = LoraConfig(\n",
    "    r=8,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=\"|\".join(controlnet_ranks.keys()),\n",
    "    rank_pattern=controlnet_ranks,\n",
    "    modules_to_save=[\"controlnet_down_blocks\", \"controlnet_mid_block\", \"controlnet_cond_embedding\", ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peft Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,965,760 || all params: 872,876,484 || trainable%: 0.7980235609142611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): UNet2DConditionModel(\n",
       "      (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_proj): Timesteps()\n",
       "      (time_embedding): TimestepEmbedding(\n",
       "        (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): SiLU()\n",
       "        (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (down_blocks): ModuleList(\n",
       "        (0): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(\n",
       "                in_features=320, out_features=320, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=320, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=320, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=320, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=320, out_features=320, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=320, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=1024, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=1024, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=320, out_features=320, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(\n",
       "                          in_features=320, out_features=2560, bias=True\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(\n",
       "                        in_features=1280, out_features=320, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(\n",
       "                in_features=320, out_features=320, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(320, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=320, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(320, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(\n",
       "                in_features=640, out_features=640, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=640, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=640, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=640, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=640, out_features=640, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=640, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=1024, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=1024, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=640, out_features=640, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(\n",
       "                          in_features=640, out_features=5120, bias=True\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=5120, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(\n",
       "                        in_features=2560, out_features=640, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(\n",
       "                in_features=640, out_features=640, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(320, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=640, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(640, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(640, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=640, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(640, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=1280, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=1280, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=1280, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=1280, out_features=1280, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=1280, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=1024, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=1024, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=1280, out_features=1280, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(\n",
       "                          in_features=1280, out_features=10240, bias=True\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(\n",
       "                        in_features=5120, out_features=1280, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(640, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1280, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1280, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1280, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DownBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1280, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1280, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (up_blocks): ModuleList(\n",
       "        (0): UpBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(2560, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1280, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=1280, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=1280, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=1280, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=1280, out_features=1280, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=1280, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=1024, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=1024, out_features=1280, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=1280, out_features=1280, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(\n",
       "                          in_features=1280, out_features=10240, bias=True\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(\n",
       "                        in_features=5120, out_features=1280, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(2560, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1280, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1920, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=1280, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1280, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(\n",
       "                in_features=640, out_features=640, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=640, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=640, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=640, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=640, out_features=640, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=640, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=1024, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=1024, out_features=640, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=640, out_features=640, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(\n",
       "                          in_features=640, out_features=5120, bias=True\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=5120, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(\n",
       "                        in_features=2560, out_features=640, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(\n",
       "                in_features=640, out_features=640, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1920, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=640, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(640, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(1280, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=640, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(640, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(960, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=640, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(640, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(\n",
       "                in_features=320, out_features=320, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=320, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=320, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=320, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=320, out_features=320, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): Linear(\n",
       "                      in_features=320, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): Linear(\n",
       "                      in_features=1024, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): Linear(\n",
       "                      in_features=1024, out_features=320, bias=False\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(\n",
       "                        in_features=320, out_features=320, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(\n",
       "                          in_features=320, out_features=2560, bias=True\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(\n",
       "                        in_features=1280, out_features=320, bias=True\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(\n",
       "                in_features=320, out_features=320, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=320, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(960, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=320, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(320, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(\n",
       "                640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(640, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (time_emb_proj): Linear(\n",
       "                in_features=1280, out_features=320, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=320, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(\n",
       "                320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Conv2d(320, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Conv2d(8, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block): UNetMidBlock2DCrossAttn(\n",
       "        (attentions): ModuleList(\n",
       "          (0): Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "      (conv_act): SiLU()\n",
       "      (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "peft_unet = get_peft_model(unet, peft_unet_conf)\n",
    "peft_unet.print_trainable_parameters()\n",
    "peft_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ModuleList:\n\tUnexpected key(s) in state_dict: \"0.attentions.0.proj_in.lora_A.default.weight\", \"0.attentions.0.proj_in.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"0.attentions.0.proj_out.lora_A.default.weight\", \"0.attentions.0.proj_out.lora_B.default.weight\", \"0.attentions.1.proj_in.lora_A.default.weight\", \"0.attentions.1.proj_in.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"0.attentions.1.proj_out.lora_A.default.weight\", \"0.attentions.1.proj_out.lora_B.default.weight\", \"0.resnets.0.conv1.lora_A.default.weight\", \"0.resnets.0.conv1.lora_B.default.weight\", \"0.resnets.0.time_emb_proj.lora_A.default.weight\", \"0.resnets.0.time_emb_proj.lora_B.default.weight\", \"0.resnets.0.conv2.lora_A.default.weight\", \"0.resnets.0.conv2.lora_B.default.weight\", \"0.resnets.1.conv1.lora_A.default.weight\", \"0.resnets.1.conv1.lora_B.default.weight\", \"0.resnets.1.time_emb_proj.lora_A.default.weight\", \"0.resnets.1.time_emb_proj.lora_B.default.weight\", \"0.resnets.1.conv2.lora_A.default.weight\", \"0.resnets.1.conv2.lora_B.default.weight\", \"1.attentions.0.proj_in.lora_A.default.weight\", \"1.attentions.0.proj_in.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"1.attentions.0.proj_out.lora_A.default.weight\", \"1.attentions.0.proj_out.lora_B.default.weight\", \"1.attentions.1.proj_in.lora_A.default.weight\", \"1.attentions.1.proj_in.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"1.attentions.1.proj_out.lora_A.default.weight\", \"1.attentions.1.proj_out.lora_B.default.weight\", \"1.resnets.0.conv1.lora_A.default.weight\", \"1.resnets.0.conv1.lora_B.default.weight\", \"1.resnets.0.time_emb_proj.lora_A.default.weight\", \"1.resnets.0.time_emb_proj.lora_B.default.weight\", \"1.resnets.0.conv2.lora_A.default.weight\", \"1.resnets.0.conv2.lora_B.default.weight\", \"1.resnets.1.conv1.lora_A.default.weight\", \"1.resnets.1.conv1.lora_B.default.weight\", \"1.resnets.1.time_emb_proj.lora_A.default.weight\", \"1.resnets.1.time_emb_proj.lora_B.default.weight\", \"1.resnets.1.conv2.lora_A.default.weight\", \"1.resnets.1.conv2.lora_B.default.weight\", \"2.attentions.0.proj_in.lora_A.default.weight\", \"2.attentions.0.proj_in.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"2.attentions.0.proj_out.lora_A.default.weight\", \"2.attentions.0.proj_out.lora_B.default.weight\", \"2.attentions.1.proj_in.lora_A.default.weight\", \"2.attentions.1.proj_in.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"2.attentions.1.proj_out.lora_A.default.weight\", \"2.attentions.1.proj_out.lora_B.default.weight\", \"2.resnets.0.conv1.lora_A.default.weight\", \"2.resnets.0.conv1.lora_B.default.weight\", \"2.resnets.0.time_emb_proj.lora_A.default.weight\", \"2.resnets.0.time_emb_proj.lora_B.default.weight\", \"2.resnets.0.conv2.lora_A.default.weight\", \"2.resnets.0.conv2.lora_B.default.weight\", \"2.resnets.1.conv1.lora_A.default.weight\", \"2.resnets.1.conv1.lora_B.default.weight\", \"2.resnets.1.time_emb_proj.lora_A.default.weight\", \"2.resnets.1.time_emb_proj.lora_B.default.weight\", \"2.resnets.1.conv2.lora_A.default.weight\", \"2.resnets.1.conv2.lora_B.default.weight\", \"3.resnets.0.conv1.lora_A.default.weight\", \"3.resnets.0.conv1.lora_B.default.weight\", \"3.resnets.0.time_emb_proj.lora_A.default.weight\", \"3.resnets.0.time_emb_proj.lora_B.default.weight\", \"3.resnets.0.conv2.lora_A.default.weight\", \"3.resnets.0.conv2.lora_B.default.weight\", \"3.resnets.1.conv1.lora_A.default.weight\", \"3.resnets.1.conv1.lora_B.default.weight\", \"3.resnets.1.time_emb_proj.lora_A.default.weight\", \"3.resnets.1.time_emb_proj.lora_B.default.weight\", \"3.resnets.1.conv2.lora_A.default.weight\", \"3.resnets.1.conv2.lora_B.default.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m peft_controlnet \u001b[38;5;241m=\u001b[39m \u001b[43mControlNetModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_unet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m peft_controlnet \u001b[38;5;241m=\u001b[39m get_peft_model(controlnet, peft_controlnet_conf)\n\u001b[1;32m      3\u001b[0m peft_controlnet\u001b[38;5;241m.\u001b[39mprint_trainable_parameters()\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/diffusers/models/controlnet.py:512\u001b[0m, in \u001b[0;36mControlNetModel.from_unet\u001b[0;34m(cls, unet, controlnet_conditioning_channel_order, conditioning_embedding_out_channels, load_weights_from_unet, conditioning_channels)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m controlnet\u001b[38;5;241m.\u001b[39mclass_embedding:\n\u001b[1;32m    510\u001b[0m         controlnet\u001b[38;5;241m.\u001b[39mclass_embedding\u001b[38;5;241m.\u001b[39mload_state_dict(unet\u001b[38;5;241m.\u001b[39mclass_embedding\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[0;32m--> 512\u001b[0m     \u001b[43mcontrolnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_blocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_blocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     controlnet\u001b[38;5;241m.\u001b[39mmid_block\u001b[38;5;241m.\u001b[39mload_state_dict(unet\u001b[38;5;241m.\u001b[39mmid_block\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m controlnet\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ModuleList:\n\tUnexpected key(s) in state_dict: \"0.attentions.0.proj_in.lora_A.default.weight\", \"0.attentions.0.proj_in.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"0.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"0.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"0.attentions.0.proj_out.lora_A.default.weight\", \"0.attentions.0.proj_out.lora_B.default.weight\", \"0.attentions.1.proj_in.lora_A.default.weight\", \"0.attentions.1.proj_in.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"0.attentions.1.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"0.attentions.1.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"0.attentions.1.proj_out.lora_A.default.weight\", \"0.attentions.1.proj_out.lora_B.default.weight\", \"0.resnets.0.conv1.lora_A.default.weight\", \"0.resnets.0.conv1.lora_B.default.weight\", \"0.resnets.0.time_emb_proj.lora_A.default.weight\", \"0.resnets.0.time_emb_proj.lora_B.default.weight\", \"0.resnets.0.conv2.lora_A.default.weight\", \"0.resnets.0.conv2.lora_B.default.weight\", \"0.resnets.1.conv1.lora_A.default.weight\", \"0.resnets.1.conv1.lora_B.default.weight\", \"0.resnets.1.time_emb_proj.lora_A.default.weight\", \"0.resnets.1.time_emb_proj.lora_B.default.weight\", \"0.resnets.1.conv2.lora_A.default.weight\", \"0.resnets.1.conv2.lora_B.default.weight\", \"1.attentions.0.proj_in.lora_A.default.weight\", \"1.attentions.0.proj_in.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"1.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"1.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"1.attentions.0.proj_out.lora_A.default.weight\", \"1.attentions.0.proj_out.lora_B.default.weight\", \"1.attentions.1.proj_in.lora_A.default.weight\", \"1.attentions.1.proj_in.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"1.attentions.1.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"1.attentions.1.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"1.attentions.1.proj_out.lora_A.default.weight\", \"1.attentions.1.proj_out.lora_B.default.weight\", \"1.resnets.0.conv1.lora_A.default.weight\", \"1.resnets.0.conv1.lora_B.default.weight\", \"1.resnets.0.time_emb_proj.lora_A.default.weight\", \"1.resnets.0.time_emb_proj.lora_B.default.weight\", \"1.resnets.0.conv2.lora_A.default.weight\", \"1.resnets.0.conv2.lora_B.default.weight\", \"1.resnets.1.conv1.lora_A.default.weight\", \"1.resnets.1.conv1.lora_B.default.weight\", \"1.resnets.1.time_emb_proj.lora_A.default.weight\", \"1.resnets.1.time_emb_proj.lora_B.default.weight\", \"1.resnets.1.conv2.lora_A.default.weight\", \"1.resnets.1.conv2.lora_B.default.weight\", \"2.attentions.0.proj_in.lora_A.default.weight\", \"2.attentions.0.proj_in.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"2.attentions.0.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"2.attentions.0.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"2.attentions.0.proj_out.lora_A.default.weight\", \"2.attentions.0.proj_out.lora_B.default.weight\", \"2.attentions.1.proj_in.lora_A.default.weight\", \"2.attentions.1.proj_in.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora_B.default.weight\", \"2.attentions.1.transformer_blocks.0.ff.net.2.lora_A.default.weight\", \"2.attentions.1.transformer_blocks.0.ff.net.2.lora_B.default.weight\", \"2.attentions.1.proj_out.lora_A.default.weight\", \"2.attentions.1.proj_out.lora_B.default.weight\", \"2.resnets.0.conv1.lora_A.default.weight\", \"2.resnets.0.conv1.lora_B.default.weight\", \"2.resnets.0.time_emb_proj.lora_A.default.weight\", \"2.resnets.0.time_emb_proj.lora_B.default.weight\", \"2.resnets.0.conv2.lora_A.default.weight\", \"2.resnets.0.conv2.lora_B.default.weight\", \"2.resnets.1.conv1.lora_A.default.weight\", \"2.resnets.1.conv1.lora_B.default.weight\", \"2.resnets.1.time_emb_proj.lora_A.default.weight\", \"2.resnets.1.time_emb_proj.lora_B.default.weight\", \"2.resnets.1.conv2.lora_A.default.weight\", \"2.resnets.1.conv2.lora_B.default.weight\", \"3.resnets.0.conv1.lora_A.default.weight\", \"3.resnets.0.conv1.lora_B.default.weight\", \"3.resnets.0.time_emb_proj.lora_A.default.weight\", \"3.resnets.0.time_emb_proj.lora_B.default.weight\", \"3.resnets.0.conv2.lora_A.default.weight\", \"3.resnets.0.conv2.lora_B.default.weight\", \"3.resnets.1.conv1.lora_A.default.weight\", \"3.resnets.1.conv1.lora_B.default.weight\", \"3.resnets.1.time_emb_proj.lora_A.default.weight\", \"3.resnets.1.time_emb_proj.lora_B.default.weight\", \"3.resnets.1.conv2.lora_A.default.weight\", \"3.resnets.1.conv2.lora_B.default.weight\". "
     ]
    }
   ],
   "source": [
    "peft_controlnet = ControlNetModel.from_unet(peft_unet)\n",
    "peft_controlnet = get_peft_model(controlnet, peft_controlnet_conf)\n",
    "peft_controlnet.print_trainable_parameters()\n",
    "peft_controlnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_controlnet.save_pretrained(\"save_controlnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gasparyanartur/dev/imaginedriving/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72074dbc87347849585dc2e0aecf41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe_peft = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
    "    model_id, unet=peft_unet, controlnet=peft_controlnet, torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m im1 \u001b[38;5;241m=\u001b[39m read_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference/pandaset-01/renders/0m/01.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m im2 \u001b[38;5;241m=\u001b[39m read_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference/pandaset-01/renders/2m/01.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \u001b[43mpipe_peft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mim1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mim2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py:1058\u001b[0m, in \u001b[0;36mStableDiffusionControlNetImg2ImgPipeline.__call__\u001b[0;34m(self, prompt, image, control_image, height, width, strength, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, controlnet_conditioning_scale, guess_mode, control_guidance_start, control_guidance_end, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m     control_guidance_start, control_guidance_end \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1053\u001b[0m         mult \u001b[38;5;241m*\u001b[39m [control_guidance_start],\n\u001b[1;32m   1054\u001b[0m         mult \u001b[38;5;241m*\u001b[39m [control_guidance_end],\n\u001b[1;32m   1055\u001b[0m     )\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# 1. Check inputs. Raise error if not correct\u001b[39;00m\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mip_adapter_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mip_adapter_image_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_guidance_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_guidance_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_on_step_end_tensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guidance_scale \u001b[38;5;241m=\u001b[39m guidance_scale\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_skip \u001b[38;5;241m=\u001b[39m clip_skip\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py:664\u001b[0m, in \u001b[0;36mStableDiffusionControlNetImg2ImgPipeline.check_inputs\u001b[0;34m(self, prompt, image, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, controlnet_conditioning_scale, control_guidance_start, control_guidance_end, callback_on_step_end_tensor_inputs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_image(image_, prompt, prompt_embeds)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# Check `controlnet_conditioning_scale`\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet, ControlNetModel)\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_compiled\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet\u001b[38;5;241m.\u001b[39m_orig_mod, ControlNetModel)\n\u001b[1;32m    671\u001b[0m ):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "im1 = read_image(\"reference/pandaset-01/renders/0m/01.jpg\").to(dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "im2 = read_image(\"reference/pandaset-01/renders/2m/01.jpg\").to(dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "pipe_peft(prompt=\"\", image=im1, mask_image=im2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_injected = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\", torch_dtype=torch.float16)\n",
    "controlnet_injected = PeftCompatibleControlNet.from_unet(unet_injected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_injected = inject_adapter_in_model(peft_unet_conf, unet_injected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet_injected = inject_adapter_in_model(peft_controlnet_conf, controlnet_injected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gasparyanartur/dev/imaginedriving/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87fa9ece3094497bdafe1763b823107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionImg2ImgPipeline {\n",
       "  \"_class_name\": \"StableDiffusionImg2ImgPipeline\",\n",
       "  \"_diffusers_version\": \"0.27.2\",\n",
       "  \"_name_or_path\": \"stabilityai/stable-diffusion-2-1\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": false,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"DDIMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "pipe_injected_base = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, unet=unet_injected.to(torch.float32), torch_dtype=torch.float32)\n",
    "pipe_injected_base.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe_injected_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m im1 \u001b[38;5;241m=\u001b[39m read_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference/pandaset-01/renders/0m/01.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, tf_pipeline\u001b[38;5;241m=\u001b[39mtransforms)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m      4\u001b[0m im2 \u001b[38;5;241m=\u001b[39m read_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference/pandaset-01/renders/2m/01.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, tf_pipeline\u001b[38;5;241m=\u001b[39mtransforms)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m \u001b[43mpipe_injected_base\u001b[49m(prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, image\u001b[38;5;241m=\u001b[39mim1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe_injected_base' is not defined"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.v2 as tvtf\n",
    "transforms = tvtf.Compose((tvtf.CenterCrop((1024, 1024)), tvtf.Resize((512, 512))))\n",
    "im1 = read_image(\"reference/pandaset-01/renders/0m/01.jpg\", tf_pipeline=transforms).to(dtype=torch.float32, device=torch.device(\"cuda\"))[None, ...]\n",
    "im2 = read_image(\"reference/pandaset-01/renders/2m/01.jpg\", tf_pipeline=transforms).to(dtype=torch.float32, device=torch.device(\"cuda\"))[None, ...]\n",
    "\n",
    "pipe_injected_base(prompt=\"\", image=im1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a95b8eb4db4e22a4f1db44c20f273a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacity of 23.67 GiB of which 100.06 MiB is free. Including non-PyTorch memory, this process has 22.51 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 176.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m pipe_injected \u001b[38;5;241m=\u001b[39m StableDiffusionControlNetImg2ImgPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     model_id, unet\u001b[38;5;241m=\u001b[39munet_injected, controlnet\u001b[38;5;241m=\u001b[39mcontrolnet_injected, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0m \u001b[43mpipe_injected\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:418\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been loaded in 8bit and moving it to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m via `.to()` is not yet supported. Module is still on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    421\u001b[0m     module\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[1;32m    425\u001b[0m ):\n\u001b[1;32m    426\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2692\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2689\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2691\u001b[0m         )\n\u001b[0;32m-> 2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 194.00 MiB. GPU 0 has a total capacity of 23.67 GiB of which 100.06 MiB is free. Including non-PyTorch memory, this process has 22.51 GiB memory in use. Of the allocated memory 22.02 GiB is allocated by PyTorch, and 176.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "pipe_injected = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
    "    model_id, unet=unet_injected, controlnet=controlnet_injected, torch_dtype=torch.float32\n",
    ")\n",
    "pipe_injected.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ModulesToSaveWrapper in module peft.utils.other object:\n",
      "\n",
      "class ModulesToSaveWrapper(torch.nn.modules.module.Module)\n",
      " |  ModulesToSaveWrapper(module_to_save, adapter_name)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ModulesToSaveWrapper\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, module_to_save, adapter_name)\n",
      " |      Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  enable_adapters(self, enabled: bool)\n",
      " |      Toggle the enabling and disabling of adapters\n",
      " |      \n",
      " |      Takes care of setting the requires_grad flag for the adapter weights.\n",
      " |      \n",
      " |      Args:\n",
      " |          enabled (bool): True to enable adapters, False to disable adapters\n",
      " |  \n",
      " |  forward(self, *args, **kwargs)\n",
      " |      Define the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  set_adapter(self, adapter_name: str)\n",
      " |      Set the active adapter\n",
      " |      \n",
      " |      Args:\n",
      " |          adapter_name (str): The name of the adapter to set as active\n",
      " |  \n",
      " |  update(self, adapter_name)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  active_adapter\n",
      " |  \n",
      " |  disable_adapters\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      # On the return type:\n",
      " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      " |      # This is done for better interop with various type checkers for the end users.\n",
      " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      " |      # See full discussion on the problems with returning `Union` here\n",
      " |      # https://github.com/microsoft/pyright/issues/4213\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Add a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
      " |      \n",
      " |      Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Return an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  compile(self, *args, **kwargs)\n",
      " |      Compile this Module's forward using :func:`torch.compile`.\n",
      " |      \n",
      " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      " |      to :func:`torch.compile`.\n",
      " |      \n",
      " |      See :func:`torch.compile` for details on the arguments for this function.\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Move all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Set the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module.\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Return any extra state to include in the module's state_dict.\n",
      " |      \n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
      " |      \n",
      " |      If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): whether to assign items in the state\n",
      " |              dictionary to their corresponding keys in the module instead\n",
      " |              of copying them inplace into the module's current parameters and buffers.\n",
      " |              When ``False``, the properties of the tensors in the current\n",
      " |              module are preserved while when ``True``, the properties of the\n",
      " |              Tensors in the state dict are preserved.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Return an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Add a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Register a post hook to be run after module's ``load_state_dict`` is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Add a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.\n",
      " |      \n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      Set extra state contained in the loaded `state_dict`.\n",
      " |      \n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`.\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Return a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Move and/or cast the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
      " |      Move the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Set the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Reset gradients of all model parameters.\n",
      " |      \n",
      " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "help(pipe_injected.controlnet.controlnet_down_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModulesToSaveWrapper(\n",
       "  (original_module): ModuleList(\n",
       "    (0-3): 4 x Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4-6): 3 x Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (7-11): 5 x Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (modules_to_save): ModuleDict(\n",
       "    (default): ModuleList(\n",
       "      (0-3): 4 x Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4-6): 3 x Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (7-11): 5 x Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_injected.controlnet.controlnet_down_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m im1 \u001b[38;5;241m=\u001b[39m read_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference/pandaset-01/renders/0m/01.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, tf_pipeline\u001b[38;5;241m=\u001b[39mtransforms)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m      4\u001b[0m im2 \u001b[38;5;241m=\u001b[39m read_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference/pandaset-01/renders/2m/01.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, tf_pipeline\u001b[38;5;241m=\u001b[39mtransforms)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m \u001b[43mpipe_injected\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mim1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mim2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py:1101\u001b[0m, in \u001b[0;36mStableDiffusionControlNetImg2ImgPipeline.__call__\u001b[0;34m(self, prompt, image, control_image, height, width, strength, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, controlnet_conditioning_scale, guess_mode, control_guidance_start, control_guidance_end, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# 3. Encode input prompt\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m text_encoder_lora_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m )\n\u001b[0;32m-> 1101\u001b[0m prompt_embeds, negative_prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_encoder_lora_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;66;03m# For classifier free guidance, we need to do two forward passes.\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# Here we concatenate the unconditional and text embeddings into a single batch\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# to avoid doing two forward passes\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_classifier_free_guidance:\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py:359\u001b[0m, in \u001b[0;36mStableDiffusionControlNetImg2ImgPipeline.encode_prompt\u001b[0;34m(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds, negative_prompt_embeds, lora_scale, clip_skip)\u001b[0m\n\u001b[1;32m    356\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip_skip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m     prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     prompt_embeds \u001b[38;5;241m=\u001b[39m prompt_embeds[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:804\u001b[0m, in \u001b[0;36mCLIPTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;124;03m>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    802\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:697\u001b[0m, in \u001b[0;36mCLIPTextTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    694\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    695\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 697\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;66;03m# CLIP's text model uses causal mask, prepare it here.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\u001b[39;00m\n\u001b[1;32m    701\u001b[0m causal_attention_mask \u001b[38;5;241m=\u001b[39m _create_4d_causal_attention_mask(\n\u001b[1;32m    702\u001b[0m     input_shape, hidden_states\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mhidden_states\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    703\u001b[0m )\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:222\u001b[0m, in \u001b[0;36mCLIPTextEmbeddings.forward\u001b[0;34m(self, input_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    219\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids[:, :seq_length]\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(position_ids)\n\u001b[1;32m    225\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeddings\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/imaginedriving/venv/lib/python3.10/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.v2 as tvtf\n",
    "transforms = tvtf.Compose((tvtf.CenterCrop((1024, 1024)), tvtf.Resize((512, 512))))\n",
    "im1 = read_image(\"reference/pandaset-01/renders/0m/01.jpg\", tf_pipeline=transforms).to(dtype=torch.float32, device=torch.device(\"cuda\"))[None, ...]\n",
    "im2 = read_image(\"reference/pandaset-01/renders/2m/01.jpg\", tf_pipeline=transforms).to(dtype=torch.float32, device=torch.device(\"cuda\"))[None, ...]\n",
    "\n",
    "pipe_injected(prompt=\"\", image=im1, control_image=im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 1080, 1920]), torch.Size([1, 3, 1080, 1920]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im1.shape, im2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
